{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('.venv')",
   "metadata": {
    "interpreter": {
     "hash": "38efbb7de01d3aab19d9ed5849c98dfab9ed16c9875c18769b7a1b3387fe98cf"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training/Valid data shape: (2115, 22, 1000)\nTest data shape: (443, 22, 1000)\nTraining/Valid target shape: (2115,)\nTest target shape: (443,)\nPerson train/valid shape: (2115, 1)\nPerson test shape: (443, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "from torchvision import transforms, utils\n",
    "import time\n",
    "# get the device type of machine\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from torchsummary import summary\n",
    "from eeg_net.solver import * \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "X_test = np.load(\"data/X_test.npy\")\n",
    "y_test = np.load(\"data/y_test.npy\")\n",
    "person_train_valid = np.load(\"data/person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"data/X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"data/y_train_valid.npy\")\n",
    "person_test = np.load(\"data/person_test.npy\")\n",
    "\n",
    "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
    "print ('Test data shape: {}'.format(X_test.shape))\n",
    "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
    "print ('Test target shape: {}'.format(y_test.shape))\n",
    "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
    "print ('Person test shape: {}'.format(person_test.shape))\n",
    "X_test_dir = './data/X_test.npy'\n",
    "y_test_dir = './data/y_test.npy' \n",
    "X_train_valid_dir = './data/X_train_valid.npy' \n",
    "y_train_valid_dir = './data/y_train_valid.npy'\n",
    "X_test_dsample_dir = './data/X_test_downsample.npy'\n",
    "y_test_dsample_dir = './data/y_test_downsample.npy' \n",
    "X_train_valid_dsample_dir = './data/X_train_valid_downsample.npy' \n",
    "y_train_valid_dsample_dir = './data/y_train_valid_downsample.npy'\n",
    "X_test_ds = np.load(X_test_dsample_dir)\n",
    "y_test_ds = np.load(y_test_dsample_dir)\n",
    "X_train_valid_ds = np.load(X_train_valid_dsample_dir)\n",
    "y_train_valid_ds = np.load(y_train_valid_dsample_dir)\n",
    "\n",
    "X_train_val_05_70_dir = './data/band_pass_data/X_train_val_05_70.npy'\n",
    "X_train_val_01_70_dir = './data/band_pass_data/X_train_val_01_70.npy' \n",
    "X_train_val_05_70_ds_dir = './data/band_pass_data/X_train_val_downsample_05_70.npy'\n",
    "X_train_val_01_70_ds_dir = './data/band_pass_data/X_train_val_downsample_01_70.npy' \n",
    "X_train_val_01_45_dir = './data/band_pass_data/X_train_val_01_45.npy'\n",
    "X_train_val_01_45_ds_dir = './data/band_pass_data/X_train_val_downsample_01_45.npy' \n",
    "\n",
    "X_train_val_05_70 = np.load(X_train_val_05_70_dir)\n",
    "X_train_val_01_70 = np.load(X_train_val_01_70_dir) \n",
    "X_train_val_05_70_ds = np.load(X_train_val_05_70_ds_dir)\n",
    "X_train_val_01_70_ds = np.load(X_train_val_01_70_ds_dir)\n",
    "y_train_valid -= 769\n",
    "y_test -= 769"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_steps(samples,samples_per_frame,stride):\n",
    "    '''\n",
    "    in:\n",
    "    samples - number of samples in the session\n",
    "    samples_per_frame - number of samples in the frame\n",
    "    stride - the gap between succesive frames\n",
    "    out: list of tuple ranges\n",
    "    '''\n",
    "    \n",
    "    i = 0\n",
    "    intervals = []\n",
    "    while i+samples_per_frame <= samples:\n",
    "        intervals.append((i,i+samples_per_frame))\n",
    "        i = i + stride\n",
    "    return intervals\n",
    "\n",
    "def make_win_data_pipeline(data_arr,label_arr,num_samples_frame,stride):\n",
    "    '''\n",
    "    in:\n",
    "    data_arr - original data array without windowing\n",
    "    label_arr - labels of the data array without windowing\n",
    "    num_samples_frame - number of samples in the frame\n",
    "    stride - the gap between succesive frames\n",
    "    \n",
    "    out:\n",
    "    data_win_arr - windowed data array\n",
    "    label_win_arr - labels of the windowed data array\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    num_trials = data_arr.shape[0]\n",
    "    num_channels = data_arr.shape[1]\n",
    "    num_samples = data_arr.shape[2]\n",
    "    \n",
    "    steps_list = make_steps(num_samples,num_samples_frame,stride)\n",
    "    num_windows = len(steps_list)\n",
    "    \n",
    "    data_win_arr = np.zeros((num_trials*num_windows,num_channels,num_samples_frame))\n",
    "    label_win_arr = []\n",
    "    k = 0\n",
    "    \n",
    "    for i in range(num_trials):\n",
    "        \n",
    "        trial_label = label_arr[i]\n",
    "        trial_data = data_arr[i,:,:]\n",
    "        \n",
    "        for m,n in enumerate(steps_list):\n",
    "            start_ind = n[0]\n",
    "            end_ind = n[1]\n",
    "            \n",
    "            win_data = trial_data[:,start_ind:end_ind]\n",
    "            data_win_arr[k,:,:] = win_data\n",
    "            label_win_arr.append(trial_label)\n",
    "            k = k+1\n",
    "    \n",
    "    label_win_arr = np.asarray(label_win_arr)\n",
    "    return data_win_arr, label_win_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the custom dataset\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    \n",
    "    \"\"\"EEG dataset\"\"\"\n",
    "    def __init__(self, subset, transform=None):\n",
    "        \n",
    "        'Initialization'\n",
    "        \n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        'Generates one sample of data'\n",
    "        \n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "          pass \n",
    "            # x = self.transform(x)\n",
    "            # y = self.transform(y)\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.subset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the shallow conv net\n",
    "\n",
    "\n",
    "class ShallowConv(nn.Module):\n",
    "    \n",
    "    # Defining the building blocks of shallow conv net\n",
    "    \n",
    "    def __init__(self, in_channels, num_conv_filters, num_samples_frame, num_eeg_channels,classes):\n",
    "    \n",
    "        # Defining as a subclass\n",
    "        super(ShallowConv, self).__init__()\n",
    "\n",
    "        self.num_samples_frame = num_samples_frame\n",
    "        self.num_conv_filters = num_conv_filters\n",
    "        self.num_eeg_channels = num_eeg_channels\n",
    "        \n",
    "        # Define the convolution layer, https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "        self.conv1 = nn.Conv2d(in_channels, self.num_conv_filters, (1, 25), stride=1)\n",
    "        self.conv_output_width =  int(self.num_samples_frame - (25-1) - 1 + 1)\n",
    "        \n",
    "        # Define the 2d batchnorm layer\n",
    "        self.bnorm2d = nn.BatchNorm2d(self.num_conv_filters)\n",
    "        \n",
    "        # Define the 1d batchnorm layer\n",
    "        self.bnorm1d = nn.BatchNorm1d(self.num_conv_filters)\n",
    "\n",
    "\n",
    "        # Define the fc layer, https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "        self.fc1 = nn.Linear(self.num_eeg_channels*self.num_conv_filters, self.num_conv_filters)\n",
    "        \n",
    "        # Define the elu activation\n",
    "        self.elu = nn.ELU(0.2)\n",
    "\n",
    "        # Define the avg pooling layer\n",
    "        self.avgpool = nn.AvgPool1d(75, stride=15)\n",
    "        \n",
    "        self.num_features_linear = int(np.floor(((self.conv_output_width - 75)/15)+1))\n",
    "        \n",
    "        \n",
    "\n",
    "        # Define the fc layer for generating the scores for classes \n",
    "        self.fc2 = nn.Linear(self.num_features_linear*self.num_conv_filters, classes)\n",
    "\n",
    "        # Define the softmax layer for converting the class scores to probabilities\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    # Defining the connections of shallow conv net\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Reshaping the input for 2-D convolution (B,22,num_samples_frame) -> (B,1,22,num_samples_frame)\n",
    "        \n",
    "        x = x.view(-1, 1, 22, self.num_samples_frame)\n",
    "        \n",
    "        # Performing the 2-D convolution (B,1,22,300) -> (B,40,22,x_shape_4dim)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x_shape_4dim = x.shape[3]\n",
    "        \n",
    "        # ELU activation\n",
    "        \n",
    "        x = self.elu(x)\n",
    "        \n",
    "        # 2d Batch normalization\n",
    "        \n",
    "        x = self.bnorm2d(x)\n",
    "        \n",
    "        \n",
    "        # Reshaping the input to dense layer (B,40,22,x_shape_4dim) -> (B,x_shape_4dim,880)\n",
    "        \n",
    "        x = x.permute(0,3,1,2) # (B,40,22,x_shape_4dim) -> (B,x_shape_4dim,40,22)\n",
    "        x = x.view(-1,x_shape_4dim,880)\n",
    "        \n",
    "        # Passing through the dense layer (B,x_shape_4dim,880) -> (B,x_shape_4dim,40)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        # ELU activation\n",
    "        \n",
    "        x = self.elu(x)\n",
    "        \n",
    "        # Square activation\n",
    "        \n",
    "        x = torch.square(x)\n",
    "        \n",
    "        # Reshaping the input for average pooling layer (B,x_shape_4dim,40) -> (B,40,x_shape_4dim)\n",
    "        \n",
    "        x = x.permute(0,2,1)\n",
    "        \n",
    "        # Passing through the average pooling layer (B,40,x_shape_4dim) -> (B,40,x_pool_3dim)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x_pool_3dim = x.shape[2]\n",
    "        \n",
    "        # Log activation\n",
    "        \n",
    "        x = torch.log(x)\n",
    "        \n",
    "        # 1D Batch normalization\n",
    "        \n",
    "        x = self.bnorm1d(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        # Reshaping the input to dense layer (B,40,x_pool_3dim) -> (B,40*x_pool_3dim)\n",
    "        \n",
    "        x = x.reshape(-1, 40*x_pool_3dim)\n",
    "        \n",
    "        # Passing through the dense layer (B,40*x_pool_3dim) -> (B,classes)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Passing through the softmax layer\n",
    "        \n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the training and validation function\n",
    "\n",
    "def train_val(model,optimizer,criterion,num_epochs):\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for phase in ['train','val']:\n",
    "            \n",
    "            \n",
    "            \n",
    "            #Initializing the losses and accuracy\n",
    "            \n",
    "            training_loss = 0\n",
    "            correct_train_preds = 0\n",
    "            total_train_preds = 0\n",
    "            batch_train_idx = 0\n",
    "            \n",
    "            validation_loss = 0\n",
    "            correct_val_preds = 0\n",
    "            total_val_preds = 0\n",
    "            batch_val_idx = 0\n",
    "            \n",
    "            \n",
    "            # Implementing the training phase\n",
    "            \n",
    "            if phase == 'train':\n",
    "                \n",
    "                # setting the model to training mode\n",
    "                \n",
    "                model.train()\n",
    "                \n",
    "                # Loading the training dataset in batches \n",
    "                \n",
    "                for inputs, labels in dataloaders['train']:\n",
    "                    \n",
    "                    # Transfer input data and labels to device\n",
    "                    \n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    \n",
    "                    # Incrementing the batch counter\n",
    "                    \n",
    "                    batch_train_idx += 1\n",
    "                    \n",
    "                    # Zeroing the gradient buffer\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Perform the forward pass\n",
    "                    \n",
    "                    outputs = model(inputs)\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    \n",
    "                    loss = criterion(outputs,labels)\n",
    "                    \n",
    "                    \n",
    "                    # Perform the backward pass\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Perform optimization step\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # Compute training statistics\n",
    "                    \n",
    "                    training_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total_train_preds += labels.size(0)\n",
    "                    correct_train_preds += predicted.eq(labels).sum().item()\n",
    "                    \n",
    "                \n",
    "                train_loss.append(training_loss)\n",
    "                t_acc = correct_train_preds/total_train_preds\n",
    "                train_acc.append(t_acc)\n",
    "                print('Training loss:',training_loss)\n",
    "                print('Training accuracy:',t_acc)\n",
    "                \n",
    "                    \n",
    "            else:\n",
    "                \n",
    "                \n",
    "                \n",
    "                # setting the model to evaluation mode\n",
    "                \n",
    "                model.eval()\n",
    "                \n",
    "                # Disable gradient computation\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    # Loading the training dataset in batches \n",
    "                    \n",
    "                    for val_inputs, val_labels in dataloaders['val']:\n",
    "                        \n",
    "                        \n",
    "                        # Transfer input data and labels to device\n",
    "                    \n",
    "                        val_inputs = val_inputs.to(device)\n",
    "                        val_labels = val_labels.to(device)\n",
    "                        \n",
    "                        # Incrementing the batch counter\n",
    "                    \n",
    "                        batch_val_idx += 1\n",
    "                        \n",
    "                        # Perform forward pass\n",
    "                        \n",
    "                        val_outputs = model(val_inputs)\n",
    "                        \n",
    "                        # Compute loss\n",
    "                        \n",
    "                        valid_loss = criterion(val_outputs,val_labels)\n",
    "                        \n",
    "                        \n",
    "                        # Compute validation statistics\n",
    "                    \n",
    "                        validation_loss += valid_loss.item()\n",
    "                        _, val_predicted = val_outputs.max(1)\n",
    "                        total_val_preds += val_labels.size(0)\n",
    "                        correct_val_preds += val_predicted.eq(val_labels).sum().item()\n",
    "                        \n",
    "                    val_loss.append(validation_loss)\n",
    "                    v_acc = correct_val_preds/total_val_preds\n",
    "                    val_acc.append(v_acc)\n",
    "                    print('Validation loss:',validation_loss)\n",
    "                    print('Validation accuracy:',v_acc)\n",
    "            \n",
    "\n",
    "            \n",
    "           \n",
    "        \n",
    "    return model, train_loss, train_acc, val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Windowed Training/Valid data shape: (2115, 22, 1000)\n",
      "Windowed Training/Valid label shape: (2115,)\n",
      "Training/Valid tensor shape: torch.Size([2115, 22, 1000])\n",
      "Training/Valid target tensor shape: torch.Size([2115])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_samples_frame = 1000\n",
    "stride = 50\n",
    "X_train_win,y_train_win = make_win_data_pipeline(X_train_valid,y_train_valid,num_samples_frame,stride)\n",
    "\n",
    "print ('Windowed Training/Valid data shape: {}'.format(X_train_win.shape))\n",
    "print ('Windowed Training/Valid label shape: {}'.format(y_train_win.shape))\n",
    "\n",
    "# Converting the numpy data to torch tensors\n",
    "\n",
    "X_train_valid_tensor = torch.from_numpy(X_train_win).float().to(device)\n",
    "y_train_valid_tensor = torch.from_numpy(y_train_win).float().long().to(device) \n",
    "\n",
    "print ('Training/Valid tensor shape: {}'.format(X_train_valid_tensor.shape))\n",
    "print ('Training/Valid target tensor shape: {}'.format(y_train_valid_tensor.shape))\n",
    "\n",
    "init_dataset = TensorDataset(X_train_valid_tensor, y_train_valid_tensor) \n",
    "\n",
    "# Spliting the dataset into training and validation\n",
    "\n",
    "lengths = [int(len(init_dataset)*0.8), int(len(init_dataset)*0.2)] \n",
    "subset_train, subset_val = random_split(init_dataset, lengths) \n",
    "\n",
    "train_data = EEGDataset(subset_train, transform=None)\n",
    "val_data = EEGDataset(subset_val, transform=None)\n",
    "\n",
    "# Constructing the training and validation dataloaders\n",
    "\n",
    "dataloaders = {\n",
    "    'train': torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=0),\n",
    "    'val': torch.utils.data.DataLoader(val_data, batch_size=8, shuffle=False, num_workers=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "Training loss: 72.47871208190918\n",
      "Training accuracy: 0.32092198581560283\n",
      "Validation loss: 70.79359459877014\n",
      "Validation accuracy: 0.3617021276595745\n",
      "Epoch 1/99\n",
      "----------\n",
      "Training loss: 69.54417634010315\n",
      "Training accuracy: 0.44089834515366433\n",
      "Validation loss: 69.58691704273224\n",
      "Validation accuracy: 0.4326241134751773\n",
      "Epoch 2/99\n",
      "----------\n",
      "Training loss: 67.3483636379242\n",
      "Training accuracy: 0.49822695035460995\n",
      "Validation loss: 68.2443071603775\n",
      "Validation accuracy: 0.4846335697399527\n",
      "Epoch 3/99\n",
      "----------\n",
      "Training loss: 65.15122604370117\n",
      "Training accuracy: 0.5667848699763594\n",
      "Validation loss: 66.39617872238159\n",
      "Validation accuracy: 0.524822695035461\n",
      "Epoch 4/99\n",
      "----------\n",
      "Training loss: 62.93272936344147\n",
      "Training accuracy: 0.6217494089834515\n",
      "Validation loss: 65.36586606502533\n",
      "Validation accuracy: 0.5555555555555556\n",
      "Epoch 5/99\n",
      "----------\n",
      "Training loss: 61.07078731060028\n",
      "Training accuracy: 0.6684397163120568\n",
      "Validation loss: 63.5584802031517\n",
      "Validation accuracy: 0.5626477541371159\n",
      "Epoch 6/99\n",
      "----------\n",
      "Training loss: 59.22658145427704\n",
      "Training accuracy: 0.7033096926713948\n",
      "Validation loss: 63.52242022752762\n",
      "Validation accuracy: 0.5555555555555556\n",
      "Epoch 7/99\n",
      "----------\n",
      "Training loss: 58.090012311935425\n",
      "Training accuracy: 0.723404255319149\n",
      "Validation loss: 61.682277381420135\n",
      "Validation accuracy: 0.6122931442080378\n",
      "Epoch 8/99\n",
      "----------\n",
      "Training loss: 56.59990257024765\n",
      "Training accuracy: 0.7624113475177305\n",
      "Validation loss: 62.36172294616699\n",
      "Validation accuracy: 0.5815602836879432\n",
      "Epoch 9/99\n",
      "----------\n",
      "Training loss: 55.759323954582214\n",
      "Training accuracy: 0.7730496453900709\n",
      "Validation loss: 60.89703834056854\n",
      "Validation accuracy: 0.6264775413711584\n",
      "Epoch 10/99\n",
      "----------\n",
      "Training loss: 54.852140724658966\n",
      "Training accuracy: 0.7848699763593381\n",
      "Validation loss: 60.98228746652603\n",
      "Validation accuracy: 0.640661938534279\n",
      "Epoch 11/99\n",
      "----------\n",
      "Training loss: 54.29879021644592\n",
      "Training accuracy: 0.8031914893617021\n",
      "Validation loss: 60.85668683052063\n",
      "Validation accuracy: 0.6288416075650118\n",
      "Epoch 12/99\n",
      "----------\n",
      "Training loss: 53.14688605070114\n",
      "Training accuracy: 0.8268321513002365\n",
      "Validation loss: 59.516258895397186\n",
      "Validation accuracy: 0.6548463356973995\n",
      "Epoch 13/99\n",
      "----------\n",
      "Training loss: 52.83888751268387\n",
      "Training accuracy: 0.8356973995271868\n",
      "Validation loss: 59.22530722618103\n",
      "Validation accuracy: 0.6643026004728132\n",
      "Epoch 14/99\n",
      "----------\n",
      "Training loss: 52.082971692085266\n",
      "Training accuracy: 0.8445626477541371\n",
      "Validation loss: 60.63445407152176\n",
      "Validation accuracy: 0.6052009456264775\n",
      "Epoch 15/99\n",
      "----------\n",
      "Training loss: 51.790576577186584\n",
      "Training accuracy: 0.8469267139479906\n",
      "Validation loss: 59.01983904838562\n",
      "Validation accuracy: 0.6501182033096927\n",
      "Epoch 16/99\n",
      "----------\n",
      "Training loss: 51.629537522792816\n",
      "Training accuracy: 0.849290780141844\n",
      "Validation loss: 58.922226786613464\n",
      "Validation accuracy: 0.6619385342789598\n",
      "Epoch 17/99\n",
      "----------\n",
      "Training loss: 50.813923835754395\n",
      "Training accuracy: 0.8711583924349882\n",
      "Validation loss: 59.59696984291077\n",
      "Validation accuracy: 0.6477541371158393\n",
      "Epoch 18/99\n",
      "----------\n",
      "Training loss: 50.21863156557083\n",
      "Training accuracy: 0.8829787234042553\n",
      "Validation loss: 59.62337625026703\n",
      "Validation accuracy: 0.6595744680851063\n",
      "Epoch 19/99\n",
      "----------\n",
      "Training loss: 50.29636961221695\n",
      "Training accuracy: 0.8847517730496454\n",
      "Validation loss: 57.964221596717834\n",
      "Validation accuracy: 0.6973995271867612\n",
      "Epoch 20/99\n",
      "----------\n",
      "Training loss: 50.122313261032104\n",
      "Training accuracy: 0.8800236406619385\n",
      "Validation loss: 58.341167628765106\n",
      "Validation accuracy: 0.6453900709219859\n",
      "Epoch 21/99\n",
      "----------\n",
      "Training loss: 49.72033244371414\n",
      "Training accuracy: 0.8930260047281324\n",
      "Validation loss: 58.16024851799011\n",
      "Validation accuracy: 0.6666666666666666\n",
      "Epoch 22/99\n",
      "----------\n",
      "Training loss: 49.61954027414322\n",
      "Training accuracy: 0.890661938534279\n",
      "Validation loss: 58.24758380651474\n",
      "Validation accuracy: 0.6713947990543735\n",
      "Epoch 23/99\n",
      "----------\n",
      "Training loss: 48.717631459236145\n",
      "Training accuracy: 0.9107565011820331\n",
      "Validation loss: 57.92101013660431\n",
      "Validation accuracy: 0.6903073286052009\n",
      "Epoch 24/99\n",
      "----------\n",
      "Training loss: 48.542203187942505\n",
      "Training accuracy: 0.9178486997635934\n",
      "Validation loss: 58.75331026315689\n",
      "Validation accuracy: 0.6572104018912529\n",
      "Epoch 25/99\n",
      "----------\n",
      "Training loss: 48.7681440114975\n",
      "Training accuracy: 0.9160756501182034\n",
      "Validation loss: 58.546788573265076\n",
      "Validation accuracy: 0.6666666666666666\n",
      "Epoch 26/99\n",
      "----------\n",
      "Training loss: 48.49473279714584\n",
      "Training accuracy: 0.9178486997635934\n",
      "Validation loss: 58.31575208902359\n",
      "Validation accuracy: 0.6619385342789598\n",
      "Epoch 27/99\n",
      "----------\n",
      "Training loss: 47.8982715010643\n",
      "Training accuracy: 0.9249408983451537\n",
      "Validation loss: 57.82296675443649\n",
      "Validation accuracy: 0.6808510638297872\n",
      "Epoch 28/99\n",
      "----------\n",
      "Training loss: 47.9723185300827\n",
      "Training accuracy: 0.9284869976359338\n",
      "Validation loss: 57.90357482433319\n",
      "Validation accuracy: 0.6997635933806147\n",
      "Epoch 29/99\n",
      "----------\n",
      "Training loss: 47.694086611270905\n",
      "Training accuracy: 0.9326241134751773\n",
      "Validation loss: 57.91228270530701\n",
      "Validation accuracy: 0.6903073286052009\n",
      "Epoch 30/99\n",
      "----------\n",
      "Training loss: 47.300035774707794\n",
      "Training accuracy: 0.9426713947990544\n",
      "Validation loss: 58.394672214984894\n",
      "Validation accuracy: 0.6666666666666666\n",
      "Epoch 31/99\n",
      "----------\n",
      "Training loss: 47.08814495801926\n",
      "Training accuracy: 0.9462174940898345\n",
      "Validation loss: 57.970897138118744\n",
      "Validation accuracy: 0.6761229314420804\n",
      "Epoch 32/99\n",
      "----------\n",
      "Training loss: 47.085171937942505\n",
      "Training accuracy: 0.9391252955082743\n",
      "Validation loss: 58.14779829978943\n",
      "Validation accuracy: 0.6808510638297872\n",
      "Epoch 33/99\n",
      "----------\n",
      "Training loss: 46.96414750814438\n",
      "Training accuracy: 0.9462174940898345\n",
      "Validation loss: 58.22224795818329\n",
      "Validation accuracy: 0.6784869976359338\n",
      "Epoch 34/99\n",
      "----------\n",
      "Training loss: 46.87859618663788\n",
      "Training accuracy: 0.9397163120567376\n",
      "Validation loss: 58.00750881433487\n",
      "Validation accuracy: 0.6855791962174941\n",
      "Epoch 35/99\n",
      "----------\n",
      "Training loss: 46.87723511457443\n",
      "Training accuracy: 0.9479905437352246\n",
      "Validation loss: 57.71057832241058\n",
      "Validation accuracy: 0.7021276595744681\n",
      "Epoch 36/99\n",
      "----------\n",
      "Training loss: 46.756206035614014\n",
      "Training accuracy: 0.9450354609929078\n",
      "Validation loss: 58.310012340545654\n",
      "Validation accuracy: 0.6879432624113475\n",
      "Epoch 37/99\n",
      "----------\n",
      "Training loss: 46.54234421253204\n",
      "Training accuracy: 0.9497635933806147\n",
      "Validation loss: 58.91032403707504\n",
      "Validation accuracy: 0.6548463356973995\n",
      "Epoch 38/99\n",
      "----------\n",
      "Training loss: 46.89648526906967\n",
      "Training accuracy: 0.9497635933806147\n",
      "Validation loss: 57.87243604660034\n",
      "Validation accuracy: 0.6879432624113475\n",
      "Epoch 39/99\n",
      "----------\n",
      "Training loss: 46.183709383010864\n",
      "Training accuracy: 0.9556737588652482\n",
      "Validation loss: 58.01481211185455\n",
      "Validation accuracy: 0.6926713947990544\n",
      "Epoch 40/99\n",
      "----------\n",
      "Training loss: 46.40873247385025\n",
      "Training accuracy: 0.9592198581560284\n",
      "Validation loss: 57.41302967071533\n",
      "Validation accuracy: 0.706855791962175\n",
      "Epoch 41/99\n",
      "----------\n",
      "Training loss: 46.00452238321304\n",
      "Training accuracy: 0.958628841607565\n",
      "Validation loss: 59.04550015926361\n",
      "Validation accuracy: 0.6548463356973995\n",
      "Epoch 42/99\n",
      "----------\n",
      "Training loss: 46.4175238609314\n",
      "Training accuracy: 0.9562647754137116\n",
      "Validation loss: 58.73608261346817\n",
      "Validation accuracy: 0.6430260047281324\n",
      "Epoch 43/99\n",
      "----------\n",
      "Training loss: 46.047027230262756\n",
      "Training accuracy: 0.960401891252955\n",
      "Validation loss: 58.55265301465988\n",
      "Validation accuracy: 0.6572104018912529\n",
      "Epoch 44/99\n",
      "----------\n",
      "Training loss: 45.93257451057434\n",
      "Training accuracy: 0.9621749408983451\n",
      "Validation loss: 58.955303966999054\n",
      "Validation accuracy: 0.6643026004728132\n",
      "Epoch 45/99\n",
      "----------\n",
      "Training loss: 45.731223583221436\n",
      "Training accuracy: 0.9651300236406619\n",
      "Validation loss: 57.73610067367554\n",
      "Validation accuracy: 0.6879432624113475\n",
      "Epoch 46/99\n",
      "----------\n",
      "Training loss: 45.84101003408432\n",
      "Training accuracy: 0.9651300236406619\n",
      "Validation loss: 57.746710896492004\n",
      "Validation accuracy: 0.6950354609929078\n",
      "Epoch 47/99\n",
      "----------\n",
      "Training loss: 45.83394259214401\n",
      "Training accuracy: 0.9645390070921985\n",
      "Validation loss: 58.42327672243118\n",
      "Validation accuracy: 0.6855791962174941\n",
      "Epoch 48/99\n",
      "----------\n",
      "Training loss: 45.93263804912567\n",
      "Training accuracy: 0.9639479905437353\n",
      "Validation loss: 60.050485014915466\n",
      "Validation accuracy: 0.6217494089834515\n",
      "Epoch 49/99\n",
      "----------\n",
      "Training loss: 45.71702307462692\n",
      "Training accuracy: 0.9663120567375887\n",
      "Validation loss: 58.624420523643494\n",
      "Validation accuracy: 0.6808510638297872\n",
      "Epoch 50/99\n",
      "----------\n",
      "Training loss: 45.256869316101074\n",
      "Training accuracy: 0.9692671394799054\n",
      "Validation loss: 59.243158757686615\n",
      "Validation accuracy: 0.6477541371158393\n",
      "Epoch 51/99\n",
      "----------\n",
      "Training loss: 45.710904002189636\n",
      "Training accuracy: 0.9692671394799054\n",
      "Validation loss: 59.963572442531586\n",
      "Validation accuracy: 0.6382978723404256\n",
      "Epoch 52/99\n",
      "----------\n",
      "Training loss: 45.986211121082306\n",
      "Training accuracy: 0.9698581560283688\n",
      "Validation loss: 59.180544674396515\n",
      "Validation accuracy: 0.6595744680851063\n",
      "Epoch 53/99\n",
      "----------\n",
      "Training loss: 45.51770198345184\n",
      "Training accuracy: 0.9704491725768322\n",
      "Validation loss: 59.263888359069824\n",
      "Validation accuracy: 0.6501182033096927\n",
      "Epoch 54/99\n",
      "----------\n",
      "Training loss: 45.59916931390762\n",
      "Training accuracy: 0.9745862884160756\n",
      "Validation loss: 58.1493656039238\n",
      "Validation accuracy: 0.6737588652482269\n",
      "Epoch 55/99\n",
      "----------\n",
      "Training loss: 45.315305292606354\n",
      "Training accuracy: 0.9722222222222222\n",
      "Validation loss: 58.60218125581741\n",
      "Validation accuracy: 0.6737588652482269\n",
      "Epoch 56/99\n",
      "----------\n",
      "Training loss: 45.38294315338135\n",
      "Training accuracy: 0.9722222222222222\n",
      "Validation loss: 58.39696103334427\n",
      "Validation accuracy: 0.6832151300236406\n",
      "Epoch 57/99\n",
      "----------\n",
      "Training loss: 45.390450060367584\n",
      "Training accuracy: 0.9739952718676123\n",
      "Validation loss: 59.38911855220795\n",
      "Validation accuracy: 0.6690307328605201\n",
      "Epoch 58/99\n",
      "----------\n",
      "Training loss: 45.209585189819336\n",
      "Training accuracy: 0.9757683215130024\n",
      "Validation loss: 58.703493535518646\n",
      "Validation accuracy: 0.6926713947990544\n",
      "Epoch 59/99\n",
      "----------\n",
      "Training loss: 44.88396066427231\n",
      "Training accuracy: 0.975177304964539\n",
      "Validation loss: 59.254399955272675\n",
      "Validation accuracy: 0.6737588652482269\n",
      "Epoch 60/99\n",
      "----------\n",
      "Training loss: 45.17097270488739\n",
      "Training accuracy: 0.9793144208037825\n",
      "Validation loss: 59.4052517414093\n",
      "Validation accuracy: 0.6643026004728132\n",
      "Epoch 61/99\n",
      "----------\n",
      "Training loss: 45.446769058704376\n",
      "Training accuracy: 0.9793144208037825\n",
      "Validation loss: 58.45483559370041\n",
      "Validation accuracy: 0.6808510638297872\n",
      "Epoch 62/99\n",
      "----------\n",
      "Training loss: 45.302862107753754\n",
      "Training accuracy: 0.973404255319149\n",
      "Validation loss: 58.35998874902725\n",
      "Validation accuracy: 0.6903073286052009\n",
      "Epoch 63/99\n",
      "----------\n",
      "Training loss: 45.151107251644135\n",
      "Training accuracy: 0.9781323877068558\n",
      "Validation loss: 57.92712759971619\n",
      "Validation accuracy: 0.7257683215130024\n",
      "Epoch 64/99\n",
      "----------\n",
      "Training loss: 45.242264211177826\n",
      "Training accuracy: 0.9793144208037825\n",
      "Validation loss: 58.827947080135345\n",
      "Validation accuracy: 0.6548463356973995\n",
      "Epoch 65/99\n",
      "----------\n",
      "Training loss: 45.52980709075928\n",
      "Training accuracy: 0.9739952718676123\n",
      "Validation loss: 59.06151831150055\n",
      "Validation accuracy: 0.6595744680851063\n",
      "Epoch 66/99\n",
      "----------\n",
      "Training loss: 45.68367916345596\n",
      "Training accuracy: 0.975177304964539\n",
      "Validation loss: 58.736003041267395\n",
      "Validation accuracy: 0.6784869976359338\n",
      "Epoch 67/99\n",
      "----------\n",
      "Training loss: 45.28549748659134\n",
      "Training accuracy: 0.9769503546099291\n",
      "Validation loss: 59.261533319950104\n",
      "Validation accuracy: 0.6619385342789598\n",
      "Epoch 68/99\n",
      "----------\n",
      "Training loss: 45.17878407239914\n",
      "Training accuracy: 0.9804964539007093\n",
      "Validation loss: 58.64887821674347\n",
      "Validation accuracy: 0.6808510638297872\n",
      "Epoch 69/99\n",
      "----------\n",
      "Training loss: 45.1009287238121\n",
      "Training accuracy: 0.9787234042553191\n",
      "Validation loss: 57.96152013540268\n",
      "Validation accuracy: 0.6903073286052009\n",
      "Epoch 70/99\n",
      "----------\n",
      "Training loss: 45.07043880224228\n",
      "Training accuracy: 0.9793144208037825\n",
      "Validation loss: 58.77989560365677\n",
      "Validation accuracy: 0.6879432624113475\n",
      "Epoch 71/99\n",
      "----------\n",
      "Training loss: 44.840865552425385\n",
      "Training accuracy: 0.9799054373522459\n",
      "Validation loss: 58.762915551662445\n",
      "Validation accuracy: 0.6997635933806147\n",
      "Epoch 72/99\n",
      "----------\n",
      "Training loss: 45.15362411737442\n",
      "Training accuracy: 0.9804964539007093\n",
      "Validation loss: 58.97438979148865\n",
      "Validation accuracy: 0.6832151300236406\n",
      "Epoch 73/99\n",
      "----------\n",
      "Training loss: 45.55621838569641\n",
      "Training accuracy: 0.9763593380614657\n",
      "Validation loss: 59.37159252166748\n",
      "Validation accuracy: 0.6643026004728132\n",
      "Epoch 74/99\n",
      "----------\n",
      "Training loss: 45.505489349365234\n",
      "Training accuracy: 0.975177304964539\n",
      "Validation loss: 58.67583453655243\n",
      "Validation accuracy: 0.6926713947990544\n",
      "Epoch 75/99\n",
      "----------\n",
      "Training loss: 45.402851819992065\n",
      "Training accuracy: 0.9799054373522459\n",
      "Validation loss: 59.395024836063385\n",
      "Validation accuracy: 0.6643026004728132\n",
      "Epoch 76/99\n",
      "----------\n",
      "Training loss: 44.93363505601883\n",
      "Training accuracy: 0.9840425531914894\n",
      "Validation loss: 59.54583638906479\n",
      "Validation accuracy: 0.6713947990543735\n",
      "Epoch 77/99\n",
      "----------\n",
      "Training loss: 44.97917264699936\n",
      "Training accuracy: 0.983451536643026\n",
      "Validation loss: 59.769680082798004\n",
      "Validation accuracy: 0.6548463356973995\n",
      "Epoch 78/99\n",
      "----------\n",
      "Training loss: 45.5302854180336\n",
      "Training accuracy: 0.9822695035460993\n",
      "Validation loss: 58.831398665905\n",
      "Validation accuracy: 0.6973995271867612\n",
      "Epoch 79/99\n",
      "----------\n",
      "Training loss: 45.56409853696823\n",
      "Training accuracy: 0.9804964539007093\n",
      "Validation loss: 59.80934298038483\n",
      "Validation accuracy: 0.6619385342789598\n",
      "Epoch 80/99\n",
      "----------\n",
      "Training loss: 45.790559470653534\n",
      "Training accuracy: 0.9810874704491725\n",
      "Validation loss: 59.62417685985565\n",
      "Validation accuracy: 0.6761229314420804\n",
      "Epoch 81/99\n",
      "----------\n",
      "Training loss: 45.710185050964355\n",
      "Training accuracy: 0.9816784869976359\n",
      "Validation loss: 59.77814036607742\n",
      "Validation accuracy: 0.6572104018912529\n",
      "Epoch 82/99\n",
      "----------\n",
      "Training loss: 44.933804631233215\n",
      "Training accuracy: 0.9869976359338062\n",
      "Validation loss: 59.464628636837006\n",
      "Validation accuracy: 0.6784869976359338\n",
      "Epoch 83/99\n",
      "----------\n",
      "Training loss: 45.10358387231827\n",
      "Training accuracy: 0.9858156028368794\n",
      "Validation loss: 59.046166241168976\n",
      "Validation accuracy: 0.6973995271867612\n",
      "Epoch 84/99\n",
      "----------\n",
      "Training loss: 45.24347460269928\n",
      "Training accuracy: 0.9822695035460993\n",
      "Validation loss: 59.56063747406006\n",
      "Validation accuracy: 0.6666666666666666\n",
      "Epoch 85/99\n",
      "----------\n",
      "Training loss: 45.33675932884216\n",
      "Training accuracy: 0.9816784869976359\n",
      "Validation loss: 60.25232404470444\n",
      "Validation accuracy: 0.6595744680851063\n",
      "Epoch 86/99\n",
      "----------\n",
      "Training loss: 45.06512290239334\n",
      "Training accuracy: 0.9869976359338062\n",
      "Validation loss: 59.74905288219452\n",
      "Validation accuracy: 0.6619385342789598\n",
      "Epoch 87/99\n",
      "----------\n",
      "Training loss: 45.008599400520325\n",
      "Training accuracy: 0.9864066193853428\n",
      "Validation loss: 59.813797414302826\n",
      "Validation accuracy: 0.6666666666666666\n",
      "Epoch 88/99\n",
      "----------\n",
      "Training loss: 45.279207944869995\n",
      "Training accuracy: 0.9864066193853428\n",
      "Validation loss: 59.26135843992233\n",
      "Validation accuracy: 0.6784869976359338\n",
      "Epoch 89/99\n",
      "----------\n",
      "Training loss: 45.443039655685425\n",
      "Training accuracy: 0.9858156028368794\n",
      "Validation loss: 59.59955596923828\n",
      "Validation accuracy: 0.6903073286052009\n",
      "Epoch 90/99\n",
      "----------\n",
      "Training loss: 45.32181453704834\n",
      "Training accuracy: 0.9846335697399528\n",
      "Validation loss: 59.05485862493515\n",
      "Validation accuracy: 0.706855791962175\n",
      "Epoch 91/99\n",
      "----------\n",
      "Training loss: 45.58411079645157\n",
      "Training accuracy: 0.9858156028368794\n",
      "Validation loss: 59.42597144842148\n",
      "Validation accuracy: 0.6761229314420804\n",
      "Epoch 92/99\n",
      "----------\n",
      "Training loss: 45.575747072696686\n",
      "Training accuracy: 0.9840425531914894\n",
      "Validation loss: 61.21299463510513\n",
      "Validation accuracy: 0.6004728132387707\n",
      "Epoch 93/99\n",
      "----------\n",
      "Training loss: 45.842928528785706\n",
      "Training accuracy: 0.9869976359338062\n",
      "Validation loss: 58.67250072956085\n",
      "Validation accuracy: 0.706855791962175\n",
      "Epoch 94/99\n",
      "----------\n",
      "Training loss: 45.375018894672394\n",
      "Training accuracy: 0.9887706855791962\n",
      "Validation loss: 58.86001706123352\n",
      "Validation accuracy: 0.6973995271867612\n",
      "Epoch 95/99\n",
      "----------\n",
      "Training loss: 45.850801050662994\n",
      "Training accuracy: 0.9858156028368794\n",
      "Validation loss: 60.03704470396042\n",
      "Validation accuracy: 0.6737588652482269\n",
      "Epoch 96/99\n",
      "----------\n",
      "Training loss: 45.38204729557037\n",
      "Training accuracy: 0.9864066193853428\n",
      "Validation loss: 59.800042152404785\n",
      "Validation accuracy: 0.6713947990543735\n",
      "Epoch 97/99\n",
      "----------\n",
      "Training loss: 45.68274658918381\n",
      "Training accuracy: 0.9869976359338062\n",
      "Validation loss: 59.8888304233551\n",
      "Validation accuracy: 0.6643026004728132\n",
      "Epoch 98/99\n",
      "----------\n",
      "Training loss: 45.27371996641159\n",
      "Training accuracy: 0.9905437352245863\n",
      "Validation loss: 59.466256737709045\n",
      "Validation accuracy: 0.7021276595744681\n",
      "Epoch 99/99\n",
      "----------\n",
      "Training loss: 45.81855261325836\n",
      "Training accuracy: 0.9881796690307328\n",
      "Validation loss: 61.04450875520706\n",
      "Validation accuracy: 0.6288416075650118\n"
     ]
    }
   ],
   "source": [
    "\n",
    "weight_decay = 0.15  # weight decay to alleviate overfiting\n",
    "shallow_model = ShallowConv(in_channels=1, num_conv_filters=40,num_samples_frame=1000,num_eeg_channels=22,classes=4).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(shallow_model.parameters(), lr = 1e-4, weight_decay=weight_decay)\n",
    "\n",
    "# Training and validating the model\n",
    "\n",
    "shallow_model,t_l,t_a,v_l,v_a=train_val(shallow_model, optimizer, criterion, num_epochs=100)"
   ]
  }
 ]
}